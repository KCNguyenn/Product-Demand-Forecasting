{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Product Demand - Analysis & Models\n",
    "**Data Source:** [Kaggle](https://www.kaggle.com/felixzhao/productdemandforecasting)\n",
    "\n",
    "**Data Description from Kaggle:** *The dataset contains historical product demand for a manufacturing company with footprints globally. The company provides thousands of products within dozens of product categories. There are four central warehouses to ship products within the region it is responsible for. Since the products are manufactured in different locations all over the world, it normally takes more than one month to ship products via ocean to different central warehouses. If forecasts for each product in different central with reasonable accuracy for the monthly demand for month after next can be achieved, it would be beneficial to the company in multiple ways.*\n",
    "\n",
    "**Objective:** *Is it possible to make forecasts for thousands of products (some of them are highly variable in terms of monthly demand) for the the month after next?* <br/>\n",
    "The latest data month is Jan 2017, thus forecast is for Mar 2017 (onwards). <br/>\n",
    "\n",
    "RMSE is used as performance metric in this forecast.\n",
    "\n",
    "**Assumptions:** <br/>\n",
    "A1/ Date refers to shipping date, not order date. Otherwise, forecast cannot be done. Shipping date indicates the time when products need to be avaiable at these warehouse. <br/>\n",
    "A2/ Order demand refers to the order quantity by customers (actual demands), not the order quantity that can be fulfilled by warehouses. <br/>\n",
    "Other assumptions will be made throughout the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packagse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import attrgetter\n",
    "#pip install pmdarima\n",
    "from pmdarima.arima import auto_arima\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product_Code        object\n",
       "Warehouse           object\n",
       "Product_Category    object\n",
       "Date                object\n",
       "Order_Demand        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"Historical Product Demand.csv\")\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ All columns are formatted as characters and need to be reformatted: <br/> \n",
    "-**Date** to datetime. <br/>\n",
    "-**Order_Demand** to numeric. Order_Demand values include negative numbers that are formated as \"(number)\". These values need to be reformatted as \"-number\" before being converted to numeric.\n",
    "\n",
    "2/ Warehouse and category information do not contribute to forecasting and will be removed. <br/>\n",
    "- **Warehouse:** As per my understanding from data description, the four central warehouses serve demands for the same region. Therefore, the forecast will be performed for total demands in the region, without consideration of warehouse information. Product quantity per warehouse will be calculated based on other information like warehouse capacity and transportation cost, which is not in the scope of this demand forecasting. <br/>\n",
    "- **Product_Category:** There is no other information to understand about product characteristics in each category. Meanwhile, the number of products per category is high, which makes visualization on data patterns of products per category impossible. Thus, it is difficult to understand products' data patterns in accordance with category number. Category column does not well explain demands in this dataset, and thus can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Date to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Format Order_Demand to numeric\n",
    "data['Order_Demand'] = data['Order_Demand'].str.replace('(','-')\n",
    "data['Order_Demand'] = data['Order_Demand'].str.replace(')','')\n",
    "data['Order_Demand'] = pd.to_numeric(data['Order_Demand'])\n",
    "\n",
    "# Drop warehouse and category columns\n",
    "data = data.drop(columns=['Warehouse','Product_Category'])\n",
    "\n",
    "# Sort data by period\n",
    "data = data.sort_values('Date').reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types\n",
      "Product_Code            object\n",
      "Date            datetime64[ns]\n",
      "Order_Demand             int64\n",
      "dtype: object\n",
      "\t\n",
      "   Product_Code       Date  Order_Demand\n",
      "0  Product_0965 2011-01-08             2\n",
      "1  Product_1724 2011-05-31           108\n",
      "2  Product_1521 2011-06-24         85000\n",
      "3  Product_1521 2011-06-24          7000\n",
      "4  Product_1507 2011-09-02          1250\n",
      "\t\n",
      "Data Dimension (1048575, 3)\n",
      "\t\n",
      "The number of products is 2160\n",
      "Period range is from 2011-01-08 00:00:00 to 2017-01-09 00:00:00\n",
      "Order Qty is from -999000 to 4000000\n",
      "\t\n",
      "Number of missing values by column [0, 11239, 0]\n",
      "All missing values are in Date column.\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Types\")\n",
    "print(data.dtypes)\n",
    "print(\"\\t\")\n",
    "\n",
    "print(data.head())\n",
    "print(\"\\t\")\n",
    "\n",
    "print(\"Data Dimension\",data.shape)\n",
    "print(\"\\t\")\n",
    "\n",
    "print(\"The number of products is\",len(data['Product_Code'].value_counts().index))\n",
    "print(\"Period range is from\",data['Date'].min(),\"to\", data['Date'].max())\n",
    "print(\"Order Qty is from\",data['Order_Demand'].min(),\"to\", data['Order_Demand'].max())\n",
    "print(\"\\t\")\n",
    "\n",
    "print('Number of missing values by column',[sum(data[i].isnull()) for i in data.columns])\n",
    "print('All missing values are in Date column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Values\n",
    "Order demands in this dataset include negative values. These can be either order adjustments or order returns. <br/>\n",
    "Let's take a closer look into this by examining some products that have negative demand values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage ranges from 0.0011126440874093195 to 82.183908045977\n"
     ]
    }
   ],
   "source": [
    "# Check percentage of negative values vs postive values\n",
    "# Extract negative values and aggregate by product\n",
    "negative = data.loc[data['Order_Demand'] < 0]\n",
    "negative_pv = pd.pivot_table(negative, values='Order_Demand',index=['Product_Code'], aggfunc=np.sum\n",
    "                            ).rename(columns={'Order_Demand':'Total_Neg'})\n",
    "\n",
    "# Extract positve values and aggregate by product\n",
    "positive = data.loc[data['Order_Demand'] > 0]\n",
    "positive_pv = pd.pivot_table(positive, values='Order_Demand',index=['Product_Code'], aggfunc=np.sum)\n",
    "\n",
    "# Add a column with corresponding total positive value by product and calculate percentage, sort = desc\n",
    "negative_pv['Total_Pos'] = positive_pv.loc[positive_pv.index.isin(negative_pv.index),]\n",
    "negative_pv['Percentage'] = abs(negative_pv['Total_Neg'])*100/negative_pv['Total_Pos']\n",
    "negative_pv = negative_pv.sort_values('Percentage', ascending = False)\n",
    "\n",
    "print(\"Percentage ranges from\",negative_pv['Percentage'].min(),\"to\",negative_pv['Percentage'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand of Product_0319\n",
      "        Product_Code       Date  Order_Demand\n",
      "159537  Product_0319 2012-10-10            12\n",
      "185920  Product_0319 2012-11-26           -10\n",
      "316157  Product_0319 2013-07-15           300\n",
      "383771  Product_0319 2013-10-30             3\n",
      "383791  Product_0319 2013-10-30             5\n",
      "385642  Product_0319 2013-10-31             5\n",
      "486803  Product_0319 2014-04-14             2\n",
      "524745  Product_0319 2014-06-19          -276\n",
      "802112  Product_0319 2015-10-08             1\n",
      "831542  Product_0319 2015-11-30             1\n",
      "\t\n",
      "Demand of Product_0568\n",
      "        Product_Code       Date  Order_Demand\n",
      "691399  Product_0568 2015-03-26             4\n",
      "700218  Product_0568 2015-04-09            20\n",
      "759382  Product_0568 2015-07-23             1\n",
      "759395  Product_0568 2015-07-23            30\n",
      "771202  Product_0568 2015-08-12             1\n",
      "771205  Product_0568 2015-08-12            95\n",
      "817602  Product_0568 2015-11-05            10\n",
      "817713  Product_0568 2015-11-05           160\n",
      "822702  Product_0568 2015-11-13          -160\n",
      "869380  Product_0568 2016-02-10             5\n"
     ]
    }
   ],
   "source": [
    "# Examine demand of several products\n",
    "print(\"Demand of Product_0319\")\n",
    "print(data.loc[data['Product_Code'] == 'Product_0319'].head(10))\n",
    "print(\"\\t\")\n",
    "print(\"Demand of Product_0568\")\n",
    "print(data.loc[data['Product_Code'] == 'Product_0568'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the examples, it can be seen that negative values indicate both order adjustment/cancellation, and returns. Reasons: <br/>\n",
    "- *Product_0319*: negative demands came separately in months after order shipment and dates refer to shipping dates (and dates when products are shipped back to warehouses in case of returns), not order dates. They are highly likely to be returns. <br/>\n",
    "- *Product_0568*: negative demands are more likely to be order cancellation, because there is another observation with the same quantiy (in an opposite sign) and on the same date. \n",
    "\n",
    "Returns can stem from different reasons, which includes returns due to wrong shipment, returns due to bad quality. Concerning returns due to bad quality, products can be either fixed for future shipment or be destroyed. <br/>\n",
    "Returns should be forecasted separately and returns forecasting results can be processed together with demand forecasting based on company's policies/practices on returns.\n",
    "\n",
    "Due to information limitation on returns, I make another assumption: <br/>\n",
    "**A3: Returned products are due to quality issues and are destroyed.** <br/>\n",
    "With this assumption, return quantities are removed from the dataset and no forecasting is necessarily made on return quantity.\n",
    "\n",
    "Negative values that indicate order cancellations will be left in the dataset and offset with other values. However, in this dataset, it's difficult to differentiate between order adjustment, order cancellation, and returns. Thus, I make one more assumption: <br/>\n",
    "**A4: When customers want to adjust order quantity, system will record an order with all same information (date, order number) as the old order, but with order quantity of opposite sign (negative value), and then create another order (with the same order number) with updated order quantity and updated date (if any)**. <br/>\n",
    "With this assumption, an observation with negative order quantity is order cancellation if there is another order for that product with the same date and same order quantity with positive value. Otherwise, the negative values are returns.\n",
    "\n",
    "The dataset is then processed to remove all returns observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Steps to remove returns:\n",
    "# This concerns Date values, thus observations with missing valuse in Date column need to be removed first.\n",
    "# Observations with missing values are extracted into a new table. Investigation into missing values will be \n",
    "# performed later.\n",
    "data_null = data.loc[data['Date'].isnull() == True,]\n",
    "data = data.dropna(how='any',axis=0)\n",
    "[sum(data[i].isnull()) for i in data.columns]\n",
    "\n",
    "# Create a column with absolute values of order_demand column\n",
    "data['Abs_Demand'] = abs(data['Order_Demand'])\n",
    "\n",
    "# Remove all rows that have the same products, dates, and absolute values of demand\n",
    "data_nodup = data.drop_duplicates(subset = ['Product_Code','Date','Abs_Demand'], keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all removed duplicates rows to a new dataframe and aggregate by product and date \n",
    "# By aggregation, cancelled orders and the corresponding orders will be offset and totalled 0. \n",
    "# In cases where the sums are lower than 0, these observations are actually returns and will be removed.\n",
    "# (For example, two returns are on the same day -> same absolute values -> extracted this dataset of duplicates.)\n",
    "duplicates = data.iloc[~data.index.isin(data_nodup.index)]\n",
    "duplicates = pd.pivot_table(duplicates, values='Order_Demand',index=['Product_Code','Date'],aggfunc=np.sum\n",
    "                           ).reset_index()\n",
    "duplicates = duplicates.loc[duplicates['Order_Demand']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Code</th>\n",
       "      <th>Date</th>\n",
       "      <th>Order_Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Product_Code, Date, Order_Demand]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all rows of duplicates in the original dataset, and drop Abs_Demand column\n",
    "# Remove all negative values\n",
    "data = data.drop_duplicates(subset = ['Product_Code','Date','Abs_Demand'], keep = False)\n",
    "data = data.drop('Abs_Demand',axis=1)\n",
    "data = data.loc[data['Order_Demand']>0]\n",
    "\n",
    "# Add duplicates table back to data\n",
    "data = pd.concat([data,duplicates], ignore_index = True)\n",
    "\n",
    "# Check to see if any negative values remain\n",
    "data.loc[data['Order_Demand'] <0] # no rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of products with null is 82\n"
     ]
    }
   ],
   "source": [
    "# The number of products have null values\n",
    "print(\"The number of products with null is\",len(data_null['Product_Code'].value_counts().index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the assumptions above, it is impossible to further analysis observations that have missing dates. \n",
    "There is no other option but to omit those observations.\n",
    "The current dataset at this step already have all null values omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate data by month\n",
    "Forecasting is performed at monthly horizons, thus the dataset should first be aggregated by month. Date is extracted with Month & Year only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Code</th>\n",
       "      <th>Period</th>\n",
       "      <th>Order_Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product_0965</td>\n",
       "      <td>2011-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product_1724</td>\n",
       "      <td>2011-05</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product_1521</td>\n",
       "      <td>2011-06</td>\n",
       "      <td>92000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product_1507</td>\n",
       "      <td>2011-09</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product_0608</td>\n",
       "      <td>2011-09</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product_Code   Period  Order_Demand\n",
       "0  Product_0965  2011-01             2\n",
       "1  Product_1724  2011-05           108\n",
       "2  Product_1521  2011-06         92000\n",
       "3  Product_1507  2011-09          1250\n",
       "4  Product_0608  2011-09             5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Date'] = data['Date'].dt.to_period('M')\n",
    "data = data.rename(columns = {\"Date\": 'Period'})\n",
    "data = data.groupby(['Product_Code','Period'])['Order_Demand'].sum().reset_index().sort_values('Period'\n",
    "            ).reset_index().drop('index',axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if periods in dataset is continuous\n",
    "# Create a duration with continuous periods\n",
    "full_period = pd.date_range('2011-01-01','2016-12-31', freq='MS').to_period('M')\n",
    "full_period = set(full_period)\n",
    "data_period = set(data['Period'])\n",
    "full_period.difference(data_period)\n",
    "# The missing periods are 5 months in 2011, including Feb, Mar, Apr, Jul, and Aug.\n",
    "# There are various possible reasons for the missing periods: No demands are in these months, warehouses to be\n",
    "# closed in these months for some reason, missing data in these periods, etc.\n",
    "# To ensure that the training data will not be misleading, all data before Sep 2011 will be removed.\n",
    "data = data.loc[data['Period'] > '2011-08']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check to see which products are eligible for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some analysis is done to see if any products should be excluded from the forecasting. <br/>\n",
    "Several criteria are as below: <br/>\n",
    "**1/** All data in Jan, 2017 should be removed; otherwise, model interpretation will be misled. <br/>\n",
    "Reason: The latest date in this dataset is 2017/01/09 while forecasting is for monthly horizon. <br/>\n",
    "**2/** Products that have no order quantities in 2016 will be removed. <br/>\n",
    "Reason: It is highly likely that the products have been stopped already and will have no future demand. The duration of 1 year helps cover the cases of seasonal products.  <br/>\n",
    "**3/** Products with demand history less than 24 months will be removed. <br/>\n",
    "Reason: A minimum of 2 years' data is required to forecast trends and seasonality using statistical forecasting methods. Also, in cases when history is too short (6 months for example), the products are likely to be new products and forecasting methods for new products are different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria 1: Remove data in Jan, 2017\n",
    "data = data.loc[data['Period']<'2017-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria 2: Remove stopped products\n",
    "latest_datamonth = data.groupby('Product_Code')['Period'].max().reset_index()\n",
    "latest_datamonth = latest_datamonth.loc[latest_datamonth['Period'] > '2015-12']\n",
    "data = data.loc[data['Product_Code'].isin(latest_datamonth['Product_Code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria 3: Remove new products\n",
    "duration_data = data.groupby('Product_Code').agg({'Period': ['min', 'max']}).reset_index()\n",
    "duration_data['Duration'] = (duration_data[('Period', 'max')] - duration_data[('Period', 'min')]\n",
    "                            ).apply(attrgetter('n')) + 1\n",
    "duration_data = duration_data.loc[duration_data['Duration'] > 24 ]\n",
    "data = data.loc[data['Product_Code'].isin(duration_data['Product_Code'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct time series in a columnar format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_0001</th>\n",
       "      <th>Product_0002</th>\n",
       "      <th>Product_0003</th>\n",
       "      <th>Product_0004</th>\n",
       "      <th>Product_0005</th>\n",
       "      <th>Product_0006</th>\n",
       "      <th>Product_0007</th>\n",
       "      <th>Product_0008</th>\n",
       "      <th>Product_0009</th>\n",
       "      <th>Product_0010</th>\n",
       "      <th>...</th>\n",
       "      <th>Product_2163</th>\n",
       "      <th>Product_2164</th>\n",
       "      <th>Product_2165</th>\n",
       "      <th>Product_2166</th>\n",
       "      <th>Product_2167</th>\n",
       "      <th>Product_2168</th>\n",
       "      <th>Product_2169</th>\n",
       "      <th>Product_2170</th>\n",
       "      <th>Product_2171</th>\n",
       "      <th>Product_2172</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Period</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-09</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-12</th>\n",
       "      <td>300.0</td>\n",
       "      <td>221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01</th>\n",
       "      <td>9700.0</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2061 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Product_0001  Product_0002  Product_0003  Product_0004  Product_0005  \\\n",
       "Period                                                                          \n",
       "2011-09           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-10           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-11           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-12         300.0      221000.0           0.0           0.0           0.0   \n",
       "2012-01        9700.0       65000.0         400.0         300.0           0.0   \n",
       "\n",
       "         Product_0006  Product_0007  Product_0008  Product_0009  Product_0010  \\\n",
       "Period                                                                          \n",
       "2011-09           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-10           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-11           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-12           0.0           0.0           0.0           0.0           0.0   \n",
       "2012-01           0.0        2100.0        4500.0         200.0         600.0   \n",
       "\n",
       "         ...  Product_2163  Product_2164  Product_2165  Product_2166  \\\n",
       "Period   ...                                                           \n",
       "2011-09  ...           0.0           0.0           0.0           0.0   \n",
       "2011-10  ...           0.0           0.0           0.0           0.0   \n",
       "2011-11  ...           0.0           0.0           0.0           0.0   \n",
       "2011-12  ...           0.0           0.0         107.0           3.0   \n",
       "2012-01  ...         101.0           4.0         115.0         137.0   \n",
       "\n",
       "         Product_2167  Product_2168  Product_2169  Product_2170  Product_2171  \\\n",
       "Period                                                                          \n",
       "2011-09           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-10           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-11           0.0           0.0           0.0           0.0           0.0   \n",
       "2011-12          20.0           4.0           2.0           2.0           0.0   \n",
       "2012-01        3844.0         463.0         509.0          18.0          31.0   \n",
       "\n",
       "         Product_2172  \n",
       "Period                 \n",
       "2011-09           0.0  \n",
       "2011-10           0.0  \n",
       "2011-11           0.0  \n",
       "2011-12           0.0  \n",
       "2012-01         100.0  \n",
       "\n",
       "[5 rows x 2061 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.pivot_table(data, values = 'Order_Demand', index = 'Period', columns = 'Product_Code',aggfunc=np.sum\n",
    "                     ).reset_index().rename_axis(\"\", axis=\"columns\")\n",
    "\n",
    "#Fill in missing values with 0. Months with missing values are implied to have zero demands.\n",
    "data = data.fillna(0)\n",
    "data = data.set_index('Period')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset includes data for 2061 products, which takes Python too long to return model results.\n",
    "# Thus, I sampled only several products with different patterns for demonstration purpose\n",
    "data = data[['Product_0002','Product_0138','Product_0597','Product_0875','Product_2066',\n",
    "               'Product_2091','Product_2127','Product_2165']]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split train and test set\n",
    "# history length of a product starts from the month of first order, not all products have the same history length\n",
    "# the function is to split train-test based on product's history length instead of dataset length\n",
    "def train_test(data):\n",
    "    myList = data.tolist()\n",
    "    i = myList.index(next(filter(lambda x: x!=0, myList)))\n",
    "    data = data.iloc[i:,]\n",
    "    train = data[:int(0.7*(len(data)))]\n",
    "    test = data[int(0.7*len(data)):]\n",
    "    return train, test\n",
    "\n",
    "# create data for forecasting\n",
    "start = data.index.tolist()[-1] + 3\n",
    "fcastperiods = 6  # forecast periods is subject to change by forecast users\n",
    "full_period = [start + x for x in range(0,fcastperiods)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# build function to run model for all columns\n",
    "Arima = ['Arima']\n",
    "ArimaFcastPerf = pd.DataFrame({'Models': Arima})\n",
    "\n",
    "ArimaData = pd.DataFrame({'Period': full_period, 'Model': 'Arima'})\n",
    "\n",
    "def arimafcast(data):\n",
    "    for i in data.columns:\n",
    "        try:\n",
    "            train, test = train_test(data[i])\n",
    "            model = auto_arima(train, start_p=0, start_q=0, m=1, trace=True, \n",
    "                               error_action='ignore', suppress_warnings=True)\n",
    "            model.fit(train)\n",
    "            pred = np.round(model.predict(n_periods=len(test)+3+fcastperiods-1))\n",
    "            ArimaFcastPerf[i] = sqrt(mean_squared_error(test,pred[:-fcastperiods-2]))\n",
    "            ArimaData[i] = pred[-fcastperiods:]\n",
    "        except:\n",
    "            ArimaFcastPerf[i] = np.nan\n",
    "            ArimaData[i] = np.nan\n",
    "    return ArimaFcastPerf, ArimaData\n",
    "arimafcast(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# build function to run model for all columns\n",
    "SES = ['SES']\n",
    "SESFcastPerf = pd.DataFrame({'Models': SES})\n",
    "\n",
    "SESData = pd.DataFrame({'Period': full_period, 'Model': 'SES'})\n",
    "\n",
    "def sesfcast(data):\n",
    "    for i in data.columns:\n",
    "        try:\n",
    "            train, test = train_test(data[i])\n",
    "            model = SimpleExpSmoothing(train).fit()\n",
    "            pred = np.round(model.forecast(len(test)+3+fcastperiods-1))\n",
    "            SESFcastPerf[i] = sqrt(mean_squared_error(test,pred[:-fcastperiods-2]))\n",
    "            SESData[i] = pred[-fcastperiods:].tolist()\n",
    "        except:\n",
    "            SESFcastPerf[i] = np.nan\n",
    "            SESData[i] = np.nan\n",
    "    return SESFcastPerf, SESData\n",
    "sesfcast(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double Exponential Smoothing\n",
    "Note: Double & Triple Exponential Smoothing only apply to positive data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# build function to run model for all columns\n",
    "DES = ['DES']\n",
    "DESFcastPerf = pd.DataFrame({'Models': DES})\n",
    "\n",
    "DESData = pd.DataFrame({'Period': full_period, 'Model': 'DES'})\n",
    "\n",
    "def desfcast(data):\n",
    "    for i in data.columns:\n",
    "        try:\n",
    "            train, test = train_test(data[i])\n",
    "            if train.min() != 0: #Double & Triple Exponential Smoothing only applies to positive data values\n",
    "                model = ExponentialSmoothing(train, trend='add', seasonal=None, damped=True\n",
    "                                            ).fit(optimized=True,use_boxcox=True)\n",
    "                pred = np.round(model.forecast(len(test)+3+fcastperiods-1))\n",
    "                DESFcastPerf[i] = sqrt(mean_squared_error(test,pred[:-fcastperiods-2]))\n",
    "                DESData[i] = pred[-fcastperiods:].tolist()\n",
    "            else:\n",
    "                DESFcastPerf[i] = np.nan\n",
    "                DESData[i] = np.nan  \n",
    "        except:\n",
    "            DESFcastPerf[i] = np.nan\n",
    "            DESData[i] = np.nan\n",
    "    return DESFcastPerf, DESData\n",
    "desfcast(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Triple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# build function to run model for all columns\n",
    "TES = ['TES']\n",
    "TESFcastPerf = pd.DataFrame({'Models': TES})\n",
    "\n",
    "TESData = pd.DataFrame({'Period': full_period, 'Model': 'TES'})\n",
    "\n",
    "def tesfcast(data):\n",
    "    for i in data.columns:\n",
    "        try:\n",
    "            train, test = train_test(data[i])\n",
    "            if train.min() != 0: \n",
    "                model = ExponentialSmoothing(train, seasonal_periods=12, \n",
    "                                             trend='add', seasonal='mul', damped=True\n",
    "                                            ).fit(optimized=True,use_boxcox=True)\n",
    "                pred = np.round(model.forecast(len(test)+3+fcastperiods-1))\n",
    "                TESFcastPerf[i] = sqrt(mean_squared_error(test,pred[:-fcastperiods-2]))\n",
    "                TESData[i] = pred[-fcastperiods:].tolist()\n",
    "            else:\n",
    "                TESFcastPerf[i] = np.nan\n",
    "                TESData[i] = np.nan       \n",
    "        except:\n",
    "            TESFcastPerf[i] = np.nan\n",
    "            TESData[i] = np.nan   \n",
    "    return TESFcastPerf, TESData\n",
    "tesfcast(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare and select the best model for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_0002</th>\n",
       "      <th>Product_0138</th>\n",
       "      <th>Product_0597</th>\n",
       "      <th>Product_0875</th>\n",
       "      <th>Product_2066</th>\n",
       "      <th>Product_2091</th>\n",
       "      <th>Product_2127</th>\n",
       "      <th>Product_2165</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Models</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arima</th>\n",
       "      <td>118217.407066</td>\n",
       "      <td>1920.982657</td>\n",
       "      <td>4.339739</td>\n",
       "      <td>408.088226</td>\n",
       "      <td>239.215384</td>\n",
       "      <td>102.328280</td>\n",
       "      <td>165.067495</td>\n",
       "      <td>83.539779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SES</th>\n",
       "      <td>158304.234195</td>\n",
       "      <td>2229.192785</td>\n",
       "      <td>2.886751</td>\n",
       "      <td>544.968806</td>\n",
       "      <td>231.796737</td>\n",
       "      <td>102.033554</td>\n",
       "      <td>165.067495</td>\n",
       "      <td>83.539779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DES</th>\n",
       "      <td>117386.637451</td>\n",
       "      <td>1415.496733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.961417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TES</th>\n",
       "      <td>192662.202227</td>\n",
       "      <td>1306.719376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.229532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Product_0002  Product_0138  Product_0597  Product_0875  Product_2066  \\\n",
       "Models                                                                          \n",
       "Arima   118217.407066   1920.982657      4.339739    408.088226    239.215384   \n",
       "SES     158304.234195   2229.192785      2.886751    544.968806    231.796737   \n",
       "DES     117386.637451   1415.496733           NaN           NaN           NaN   \n",
       "TES     192662.202227   1306.719376           NaN           NaN           NaN   \n",
       "\n",
       "        Product_2091  Product_2127  Product_2165  \n",
       "Models                                            \n",
       "Arima     102.328280    165.067495     83.539779  \n",
       "SES       102.033554    165.067495     83.539779  \n",
       "DES              NaN           NaN     72.961417  \n",
       "TES              NaN           NaN    108.229532  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine all performance tables\n",
    "FcastPerf = pd.concat([ArimaFcastPerf,SESFcastPerf,DESFcastPerf,TESFcastPerf]).set_index('Models')\n",
    "FcastPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_0002</th>\n",
       "      <th>Product_0138</th>\n",
       "      <th>Product_0597</th>\n",
       "      <th>Product_0875</th>\n",
       "      <th>Product_2066</th>\n",
       "      <th>Product_2091</th>\n",
       "      <th>Product_2127</th>\n",
       "      <th>Product_2165</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DES</td>\n",
       "      <td>TES</td>\n",
       "      <td>SES</td>\n",
       "      <td>Arima</td>\n",
       "      <td>SES</td>\n",
       "      <td>SES</td>\n",
       "      <td>Arima</td>\n",
       "      <td>DES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Product_0002 Product_0138 Product_0597 Product_0875 Product_2066  \\\n",
       "0          DES          TES          SES        Arima          SES   \n",
       "\n",
       "  Product_2091 Product_2127 Product_2165  \n",
       "0          SES        Arima          DES  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find best model for each product\n",
    "Model = pd.DataFrame()\n",
    "\n",
    "for i in FcastPerf.columns:\n",
    "    # Find the one with lowest RMSE. Choose the first one in case of more than 1 min values.\n",
    "    Model[i] = list([FcastPerf.loc[FcastPerf[i] == FcastPerf[i].min()].index[0]])\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arima': ['Product_0875', 'Product_2127'],\n",
       " 'SES': ['Product_0597', 'Product_2066', 'Product_2091'],\n",
       " 'DES': ['Product_0002', 'Product_2165'],\n",
       " 'TES': ['Product_0138']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter list of products per model\n",
    "model_dict = {}\n",
    "model_list = ['Arima', 'SES', 'DES', 'TES']\n",
    "for i in model_list:\n",
    "    model_dict[i] = []\n",
    "\n",
    "for i in Model.columns:\n",
    "    for j in model_list:\n",
    "        if any(Model[i] == j):\n",
    "            model_dict[j].append(i)\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract forecast from equivalent models to the final forecast dataframe\n",
    "AllForecast = pd.concat([ArimaData, SESData, DESData, TESData])\n",
    "FinalForecast = pd.DataFrame({'Period': full_period})\n",
    "\n",
    "for i in model_list:\n",
    "    for j in model_dict[i]:\n",
    "        FinalForecast[j] = AllForecast.loc[AllForecast['Model'] == i][j]\n",
    "FinalForecast = FinalForecast[sorted(FinalForecast)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Product_0002</th>\n",
       "      <th>Product_0138</th>\n",
       "      <th>Product_0597</th>\n",
       "      <th>Product_0875</th>\n",
       "      <th>Product_2066</th>\n",
       "      <th>Product_2091</th>\n",
       "      <th>Product_2127</th>\n",
       "      <th>Product_2165</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03</td>\n",
       "      <td>138027.0</td>\n",
       "      <td>3861.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04</td>\n",
       "      <td>138027.0</td>\n",
       "      <td>2063.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05</td>\n",
       "      <td>138027.0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>138027.0</td>\n",
       "      <td>2547.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07</td>\n",
       "      <td>138027.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-08</td>\n",
       "      <td>138027.0</td>\n",
       "      <td>3181.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period  Product_0002  Product_0138  Product_0597  Product_0875  \\\n",
       "0  2017-03      138027.0        3861.0           3.0         204.0   \n",
       "1  2017-04      138027.0        2063.0           3.0         204.0   \n",
       "2  2017-05      138027.0        1894.0           3.0         204.0   \n",
       "3  2017-06      138027.0        2547.0           3.0         204.0   \n",
       "4  2017-07      138027.0        1725.0           3.0         204.0   \n",
       "5  2017-08      138027.0        3181.0           3.0         204.0   \n",
       "\n",
       "   Product_2066  Product_2091  Product_2127  Product_2165  \n",
       "0          19.0          94.0         182.0          98.0  \n",
       "1          19.0          94.0         182.0          98.0  \n",
       "2          19.0          94.0         182.0          98.0  \n",
       "3          19.0          94.0         182.0          98.0  \n",
       "4          19.0          94.0         182.0          98.0  \n",
       "5          19.0          94.0         182.0          98.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final forecast\n",
    "#FinalForecast.to_csv(\"FinalForecast.csv\")\n",
    "FinalForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of this forecast:** <br/>\n",
    "1/ *Scattered data*: The data scattered too much, and not really appropriate for statistical forecasting methods. That might be the result that the forecasting results do not look really good. <br/>\n",
    "2/ *No external factors explained:* The forecast is based on historical data only and doesnt incorporate any external information like economic conditions, product price policy changes, future promotions, etc. <br/>\n",
    "3/ *No consideration of returns impact:* Order returns can make negative impact on future demand. For example, customers may leave, which leads to a drop in future demand. However, there is not information to make any assumptions about this impact in this forecasting. <br/>\n",
    "4/ *Omitted missing values:* It's impossible to measure the impact of removing missing values in this dataset. <br/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
